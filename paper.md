**NOTE: I realized that I uninstalled latex on my mac because I ran out of space, so writing as markdown here, somebody else convert to latex**

Advances in computing technology for ad-hoc pedagogy
---

Abstract
---
There is an increasing interest in learning outside of the traditional classroom setting, especially for instruction of computational tools and practices that are challenging to incorporate in the standard curriculum. These atypical learning environments offer new ways for teaching students skills and concepts, particularly when it comes to combining conceptual knowledge with hands-on methods. Advances in cloud computing and containerized environments offers an attractive opportunity to improve the efficiency and easy with which students can learn. This manuscript details recent advances towards using commonly-available cloud computing services for improving the student experience in bootcamp-style events. We cover the benefits (and challenges) with using a server hosted remotely instead of relying on student laptops, discuss the technology that was used in order to make this possible, and give suggestions for how others could implement and improve upon this model for pedagogy.


Body
---
# Introduction
Over the last several years, there has been a growing interest in learning outside of the traditional classroom setting. Whether it is because departments do not offer classes in a certain area, or because the format of semester-long classes does not mesh well with the topics covered, there is a demand for new approaches to teaching the skills and ideas needed to do science. One common way to teach new skills is a short, time-bounded learning event, such as a bootcamp or short-course. These attempt to compress several topics into an intensive learning session that is usually held over one or several days.

These kinds of time-bounded bootcamps offer many advantages for learning over longer and less-frequent courses. For example, they allow the students to focus entirely one one topic for an extended period of time. This can be particularly useful for material that demands a "deeper dive" and intensive working. It is also particularly useful for topics that attempt to jointly cover both conceptual material and more "hands-on" material, because the increased time leaves more room for experimentation, discussion, and active learning. Finally, the opt-in nature of these courses often ensures that students are more motivated to learn, and their proximity both to one another as well as to the instructors makes for a good learning environment.

A common event of this nature often follows the same rough formula. First, instructors develop materials on their own computers, perhaps eventually sharing them together in the same github repository. On the days preceding the event, instructors send links to github repositories, as well as instructions for how to download the proper packages and computing environments. On the day of, instructors assume that students have already figured out how to install this material on their computers, or often hold mini "startup" sessions that assist students that have problems in getting their environments set up. During the bootcamp, material is covered that was developed using the instructor computers, meaning that differences in computing power, memory, etc will cause some students to have failed kernels or exceptionally slow execution. Finally, students are then sent home with their newly-acquired knowledge and all course materials they used during the class. This has the benefit of being immediately usable outside of class (assuming that course materials were able to run during class). Course materials generally remain on github, though they have a relatively high barrier to entry for new students to discover because of the aforementioned environment setup costs.

These types of courses also offer unique challenges in effectively teaching students. Because these courses emphasize hands-on learning in which students perform interact with material on their own, the course experience becomes heavily-dependent on the ability of each student to get started in the first place. In addition, asking students to run material on their own hardware limits and adds variability to the types of computations that can be covered in the class. Finally, because interacting with course materials requires installing many pieces of custom software, it is rare for course materials to be re-utilized by new users after the course has completed.

One solution for handling these challenges is to offload the challenge of student-specific hardware onto a shared cloud computing platform. This approach standardizes the experience of each student by allowing them access to a single online resource for the duration of the class. This article covers a recent attempt at using advanced cyberinfrastructure in order to hold a day-long bootcamp in machine learning at the University of California in San Francisco. We will cover the technological advancements necessary in order to host course materials online and make them available to students, as well as cover the challenges faced in implementing this course setup effectively.

# Course overview and development
The workshop focused on the analysis and interpretation of neuroimaging data ranging from whole-brain functional magnetic resonance imaging (fMRI) to single-cell microscopy. Instructors were distributed across three institutions, and attendees were mostly graduate students and post-docs that had minimal background in data analysis and image processing. The instructors took the audience through detailed hands-on data analysis pipelines that harness open source software for image processing  (http://scikit-image.org/, http://nipy.org/dipy/). They also introduced the participants to machine learning techniques (e.g., deep learning methods using Caffe and Tensorflow as well an introduction to scikit-learn).

*** IMAGE OF JUPYTER ENVIRONMENT IN CLASS ***

All course materials were hosted via a shared online computing platform. This made it possible to standardize the computing environment across both students and instructors, and minimized the effort needed to get students started with the material. In order to make course materials accessible in live, online computing environments, the following considerations were taken.

## Software and computing environment
First, a docker image was created with computational restrictions and a computing environment that would match what would be available on the cluster. Instructors used a shared Docker image in order to share this environment as they developed independently. They used a shared github repository to host each section of the bootcamp, and pushed their materials as separate folders in this repository. As new packages were needed in order to cover particular topics, the base Docker image was updated with new dependencies added. As a result, by the end of course development there was a single Docker image with all software tools needed in order to complete the course.

## Data management
Another challenge with hosting bottcamp-style events entails handling data that is often cumbersome to download and modify. This course covered a wide range of datasets including publicly-available functional magnetic resonance imaging (fMRI) data as well as images of cells collected from the human brain. Instructors used a script that fetched this data from online repositories, and then stored it in a common data folder on the user's computer. The Docker images were then configured such that this script would be run as soon as a new image was instantiated, ensuring that all data was pre-loaded onto the user's filesystem upon launch. In addition, because students had access to their own computing environments, this data (and any modifications of it) persisted over time and computing sessions.

## Environment distribution
As all development of the course was performed in Docker containers, it ensured that all course materials could run on any computing architecture that could host such a Docker image. In the day before the class, this image was deployed to several instances being run on a cloud computing platform called XSEDE (see technical infrastructure section). A single IP was generated for each instance - one for each student - and was sent out the morning of the event. Students only needed to click on their respective IP address, and they were instantly taken to a live jupyter notebook contained within their XSEDE computing environment. This notebook had all course materials, data, software packages, and live python kernels needed to complete the course. In addition, this environment persisted several days after the class was finished in order to allow students to continue interacting with materials, or to port their work onto their personal computers.

*** FIGURE OF WORKFLOW DIAGRAM ***

# Comparisons with a traditional teaching setup
This section covers some of the main benefits and drawbacks of the cloud computing approach towards bootcamp-style events. First we will cover primary benefits of a traditional bootcamp event, next we will discuss drawbacks and how these are addressed by using cloud infrastructure. Finally we will discuss new challenges that were introduced and how they might be addressed in the future.

## Benefits of a traditional course structure
The messiness of asking students to do all work on their own machines has one primary benefit: they are operating in the computing environment that they likely use on a daily basis. While it is common for frustrations to pop up during installation and execution of course materials, these also serve as learning opportunities for the frustrations that even seasoned coders experience on a daily basis. In addition, by doing all course computation on their own computer, there is no need to adapt to a completely new computing approach (beyond learning about new packages, languages, etc). Finally, after the course is over there is no need to migrate course materials to new hardware, and students may theoretically continue interacting with material immediately.

## Challenges of a traditional course structure
The primary drawback of this approach comes in the form of variability and reliability. Because students bring their own computing environments with them, the experience with the course depends heavily on whether materials were successfully able to run on their laptops. For example, some students have difficulty setting up packages and environments, this costs time at the beginning of class and often creates interruptions throughout the day. In addition, some have pre-existing installations of software that may clash with new installations, or introduce version dependency clashes. Finally, because computational power varies across students, it limits the kind of tutorials that can be given by the instructor. For example, one must limit the computational load required to perform any analyses in class. This has implications for equality and discrimination, as students without access to laptops with the proper hardware for completing the course are often frustrated and unable to learn as effectively.

Another challenge with this approach is in the ability (or lack thereof) for instructors to control the software experience of each student. For example, because operating systems have different file system structures, there may be broken paths or incorrect function calls in the course material. In addition because students will pull the material for the day onto their own computers, the material must be frozen relatively early. It is complicated for instructors to update materials on the day-of unless they want to lead the class through a joint session on pulling from github.

## Benefits of the cloud-computing approach
As discussed above, the key benefit of using a cloud computing approach is easily standardizing the experience of each student via a shared online resource. Because computing environments are initialized automatically with Docker and the XSEDE cluster, students incur virtually zero startup cost before interacting with the course materials. They only need a web browser in order to run jupyter notebooks. In addition, this ensured that all students had a "clean" computing environment - they had access only to the data, scripts, and packages that were required for the class.

This approach afforded many benefits for instructors and course development as well. By utilizing Docker for student deployments, the instructors knew the computational resources that would be available to the students. As such, they could scale the demands of their scripts accordingly, such that everything would "just work" once dozens of students simultaneously attempted to run their code. By hosting student materials on the cloud, it also allowed instructors access to these computing environments during the class itself. Any updates, changes to data, or new scripts could be push silently via the XSEDE platform, which reduced the headache associated with asking students to download new data. This all served to allow instructors and students to focus on the primary goal of the event: covering course material and getting students familiar with the domain-specific analysis.

An extra benefit of this approach lies in the ability to perfectly replicate the course on any new computing environment. With a few Docker commands, it would be possible to recreate the user experience on a person's laptop, provided that they had the hardware capacity to handle course computations and a minimal understanding of the Docker environment. This adds to the reproducibility of the course, and lowers the barrier to entry for users to discover and interact with the materials in the future. It also makes it easier for instructors to build new materials off of the course Docker images, which encourages collaboration and reduces the tendency for instructors to repeat efforts.

## Challenges of the cloud-computing approach and areas for improvement
While using a cloud-computing approach for bootcamp-style pedagogy provided many benefits, it also uncovered new challenges in development and execution of the course. This section covers a few specific topics that should be addressed in future iterations of this approach towards teaching.

### Knowledge in cloud computing infrastructure
The primary drawback of this approach is the necessity of knowledge in computing infrastructures and their availability. While there are many resources out there for understanding the cloud computing hardware available to researchers, it is often idiosyncratic and inaccessible for novice users. Course instructors had to work with a campus cyberinfrastructure expert in order to set up the Docker images within the XSEDE infrastructure. In addition, significant development had to go into creating and debugging the creation of Docker images, particularly early on in the development cycle of the class. In the future, it will be important to provide practical guides in interfacing with campus cloud computing architecture, as well as a well-explained workflow for how to leverage these resources in teaching.

### Expenses associated with cloud computing
Another challenge of using cloud computing is the fact that one must pay for computation time. Fortunately this is a relatively minimal cost, and many cloud computing platforms (particularly those associated with universities) are willing to give out free credits for educational purposes. There should be a set of resources to guide instructors in acquiring the computing credits needed to provide to their students. For situations where instructors do not have access to free credits for cloud computing, universities should provide modest grants to pay for these computing resources.

### Knowledge in container technology
While containerization has become commonplace in industry, it is still rarely used in academic research. Usecases such as these demonstrate its value, and we believe that services such as Docker will proliferate in academic workflows. While Docker is extremely powerful, it is still growing in both its features and API. This can be an impediment to getting up-and-running with a new install of Docker, and towards initializing an image from Dockerhub or a Dockerfile. In order to improve the accessibility of this approach for new instructors, it will be crucial to minimize the effort required to set up a minimal computational environment with Docker. Fortunately many materials such as the Jupyter base Docker image exist, though more efforts towards streamlining this process are neded.

### Migrating students from the cloud to their computers
A final challenge worth noting is the fact that students aren't actually working on their own laptops. Because all course materials are run in the cloud, it creates an extra migration step needed to do work without access to cloud computing environments. Fortunately, it is straightforward to initialize the course Docker image with a few simple commands, but this can be a large impediment for a student who has just learned to code. We recommend including guides specifically for students who wish to migrate their work onto their laptops, or holding special sessions to "off-board" students after the class is finished. This serves as a natural parralel to the "install-fest" that often happens at the *beginning* of traditional bootcamps. However, in this case the offboarding progress occurs after significant learning and experience in software and computing has been gained, potentially mitigating the frustration associated with migrating off of the cloud. On the other hand, students are now somewhat familiar with what it means to develop in a cloud computing environment, and as a result they are more poised to do their own work on cloud computing environments provided by their university (a practice that is becoming more and more common).

# Conclusions
This project represents a first step towards a flexible and easy way to deploy computational environments on existing cloud infrastructure for the purposes of teaching data analytic methods to scientists. The purpose of this manuscript is to provide inspiration and a rough guide for how one might deploy a similar approach towards teaching a bootcamp-style event. It also aims to lay out a path towards refining this process to accommodate new research domains and training events, and to make it more straightforward for instructors to set up course infrastructure without the need for exceptional technical knowledge. Utilizing cloud computing infrastructure has the ability to improve both the teaching and learning experience in data-heavy fields, and offers new opportunities for giving researchers a pragmatic, hands-on experience with data that focuses on the topics covered in the course. As the materials available for instructors improves, we believe that this approach will increase in efficacy and become a common tool in the toolkit for modern-day pedagogy.

*** THIS SECTION I'M TREATING AS METHODS-Y, IT SHOULD HAVE AS MUCH DETAIL AS POSSIBLE***

# Methods and technical infrastructure
This section describes the Advanced Cyberinfrastructure (ACI) support that made this workshop possible, and highlights reusable and shareable patterns to build on for future work.

## Computational resources
The Jetstream cloud platform provided the computational resources for the workshop. Jetstream’s core capabilities include the ability to create interactive Virtual Machines (VMs), access to remote desktops through a web browser, and publishing VMs with a DOI. Jetstream is attractive to communities who have not been users of traditional HPC systems, but who would benefit from advanced computational capabilities.

*** DETAILED DIAGRAM OF HARDWARE + SOFTWARE ***
Access to Jetstream is available to researchers at no cost through the NSF-funded XSEDE (Extreme Science and Engineering Discovery Environment) project which offers a portfolio of supercomputers and high-end visualization and data-analysis resources across the country to address increasingly diverse scientific and engineering challenges.

To obtain access, a qualified PI writes a resource justification and submits an allocation request. To help speed up the process of choosing and obtaining access to the resource, many campuses have local XSEDE Campus Champions who can facilitate quick access and help prepare an allocation request.

For the neuroimaging workshop, the local Campus Champion worked with BIDS and eScience data scientists to prepare an Education Allocation request. Below are some key excerpts from the 1-page allocation request, which you can read in full from the list of example allocation requests:

50 Virtual Machines running simultaneously (40 students + 5 instructors + test/spare/debug VMs)
Each VM will need to be a: Jetstream m1.medium VM (6 vCPUs, 16GB RAM, 60GB Storage)
Each VM will need an external IP address so students can connect remotely with a web browser to a Jupyter Notebook running on the machine
We are requesting 10,000SUs in total.
The technology we used to deploy the workshop in addition to the Jetstream cloud platform includes Docker, Dockerhub, and the docker-stacks maintained by the Jupyter project.

# Development and environment control
Each of the instructors initially used their own laptops to develop Jupyter Notebook-based tutorials on computer vision and machine learning for neuroscience, using state-of-the-art deep learning methods and software such as Tensorflow and scikit-learn.

Research IT staff worked with BIDS and eScience data scientists to build a customized container from the Jupyter project’s datascience-notebook image. This provides a pre-configured Jupyter Notebook 4.3.x; Conda Python 3.x and Python 2.7.x environments; and several common libraries including: pandas, matplotlib, scipy, seaborn, scikit-learn, and scikit-image. Additional neuroscience-specific packages were included such as Dipy for diffusion magnetic resonance imaging (dMRI) analysis.

This customized container ensured that each student had an identical environment on the day of the workshop, including all required software dependencies. The container made it possible for each participant to easily run the software without installing each of the components, often a lengthy and error-prone process at the start of many workshops. The container can also be used as a snapshot-in-time or a “time capsule” so the software is preserved for future use. Months or years for now it is possible to re-run the notebooks again, even if external software packages and dependencies have changed.

The container image was pushed to https://hub.docker.com/ which provides a centralized resource for container image discovery, distribution and change management, user and team collaboration, and workflow automation.

## Putting it together
While it is possible to download and run a customized container directly on a participant’s laptop, the instructors wanted to simplify the workshop experience. A Docker container for each participant was therefore provisioned to a virtual machine (VM) running remotely on the XSEDE Jetstream cloud platform. This allowed instructors to dive directly into the material, without a download-and-install step. The participant only needed to connect to their assigned IP address and type in a password provided on the whiteboard.

After the workshop the participants were allowed to continue accessing their notebook on the Jetstream platform for a limited time using the Education Allocation for the workshop. After the allocation expired, each individual could either:

* install Docker for Mac or Docker for Windows to download and run the container on their own laptop
* apply for their own Startup and Research Allocations on XSEDE Jetstream
